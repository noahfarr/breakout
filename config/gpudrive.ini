[base]
package = gpudrive
env_name = gpudrive
policy_name = Policy
rnn_name = Recurrent

[env]
num_worlds = 512

[train]
total_timesteps = 20_000_000
num_envs = 1
num_workers = 1
env_batch_size = 1
zero_copy = False
batch_size = 262_144
update_epochs = 3
minibatch_size = 32768
bptt_horizon = 4
anneal_lr = False
gae_lambda = 0.6936176980736478
gamma = 0.939971286862982
clip_coef = 0.15868789595955315
vf_coef = 0.8302286131303522
vf_clip_coef = 0.02976032959496967
max_grad_norm = 0.8081099987030029
ent_coef = 0.005292471910533627
learning_rate = 0.0008870638746281456
checkpoint_interval = 1000
device = cuda

[sweep.metric]
goal = maximize
name = environment/goal_achieved

[sweep.parameters.env.parameters.num_worlds]
distribution = uniform
min = 16 
max = 512

[sweep.parameters.env.parameters.max_episode_steps]
distribution = uniform
min = 100
max = 1000

[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 20_000_000
max = 25_000_000

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 32768
max = 524288

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 2048
max = 32768


